---
title: "Researching \"Musical Copycats\""
author: "Bart de Rooij"
date: "February-March 2020"
output: 
  flexdashboard::flex_dashboard:
    orientation: rows
    theme: lumen
---

```{r setup}
# In order to use these packages, we need to install flexdashboard, plotly, and Cairo.
library(tidyverse)
library(ggplot2)
library(spotifyr)
library(tidyverse)
library(flexdashboard)
library(compmus)
library(grid)
library(gridExtra)
#source('spotify.R')
```

```{r echo = FALSE}
# Set Spotify access variables (every time)
Sys.setenv(SPOTIFY_CLIENT_ID = '29b7536b5f6f4f21a1e9075195a34105')
Sys.setenv(SPOTIFY_CLIENT_SECRET = 'e5d4ec6492a44dbf808852fb35d36923')
```


New Content {.storyboard}
============================================================================================
### Comparing slow blues performances

This week I took a look at three live performances of slow blues. The performances that were chosen are "Red House" by Jimi Hendrix, "Lenny" by Stevie Ray Vaughan and "Out of My Mind" by John Mayer. All these songs were performed and written by the artists themselves so it is a good way to compare them. "Redhouse" has a bpm of approximately 55, "Lenny" has a bpm of 45 and "Out of My Mind" has a bpm of 40. <br><br>

<iframe src="https://open.spotify.com/embed/track/3Vs6cdkM3qIWItjoiiI5p2" width="333" height="380" frameborder="0" allowtransparency="true" allow="encrypted-media"></iframe>

<iframe src="https://open.spotify.com/embed/track/5w8Ti9LEgtN2wXnJ4W4qpI" width="333" height="380" frameborder="0" allowtransparency="true" allow="encrypted-media"></iframe>

<iframe src="https://open.spotify.com/embed/track/5WMKS1iDfugyLhfibIlR51" width="333" height="380" frameborder="0" allowtransparency="true" allow="encrypted-media"></iframe>

### Red House does not have a clear tempogram {data-commentary-width=320}

<div>
<h3>Tempogram Red House (Live at The Woodstock Music & Art Fair, August 18, 1969)</h3>
```{r, out.width="100%", fig.width=15}
redhouse <- 
  get_tidy_audio_analysis('3Vs6cdkM3qIWItjoiiI5p2')

tempo_redhouse <- redhouse %>% 
  tempogram(window_size = 8, hop_size = 1, cyclic = FALSE, bpms=30:150) %>% 
  ggplot(aes(x = time, y = bpm, fill = power)) + 
  geom_raster() + 
  scale_fill_viridis_c(guide = 'none') +
  labs(x = 'Time (s)', y = 'Tempo (BPM)') +
  theme_classic()

tempo_redhouse
```
</div>


***

<iframe src="https://open.spotify.com/embed/track/3Vs6cdkM3qIWItjoiiI5p2" width="300" height="380" frameborder="0" allowtransparency="true" allow="encrypted-media"></iframe>

We can see from this tempogram that the bpm cannot be clearly read from the plot. Red House has a bpm of 55 but in the plot a line around 110 can be seen. 

### Lenny slows down mid song but speeds up slightly at the end {data-commentary-width=320}

<div>
<h3>Tempogram Lenny (Live at The Spectrum, Montreal; August 17 1984)(Late Show)</h3>
```{r, out.width="100%", fig.width=15}
lenny <- 
  get_tidy_audio_analysis('5w8Ti9LEgtN2wXnJ4W4qpI')

tempo_lenny <- lenny %>% 
  tempogram(window_size = 8, hop_size = 1, cyclic = FALSE, bpms=30:150) %>% 
  ggplot(aes(x = time, y = bpm, fill = power)) + 
  geom_raster() + 
  scale_fill_viridis_c(guide = 'none') +
  labs(x = 'Time (s)', y = 'Tempo (BPM)') +
  theme_classic()

tempo_lenny
```
</div>


***

<iframe src="https://open.spotify.com/embed/track/5w8Ti9LEgtN2wXnJ4W4qpI" width="300" height="380" frameborder="0" allowtransparency="true" allow="encrypted-media"></iframe>

With this performance of Lennny some clear lines can be seen. A line around 100 and 45 can be seen which gradually goes down and back up again. This clear line is because there are very clear drums in this performance. The change of tempo correlates with the energy of the solo by Stevie Ray Vaughan.

### Out of My Mind clearly shows the tempo octaves and not the right bpm {data-commentary-width=320}

<div>
<h3>Tempogram Out of My Mind (Live at the Nokia Theatre, Los Angeles, CA - December 2007)</h3>
```{r, out.width="100%", fig.width=15}
outof <- 
  get_tidy_audio_analysis('5WMKS1iDfugyLhfibIlR51')

tempo_outof <- outof %>% 
  tempogram(window_size = 8, hop_size = 1, cyclic = FALSE, bpms=30:150) %>% 
  ggplot(aes(x = time, y = bpm, fill = power)) + 
  geom_raster() + 
  scale_fill_viridis_c(guide = 'none') +
  labs(x = 'Time (s)', y = 'Tempo (BPM)') +
  theme_classic()

tempo_outof
```

</div>


***

<iframe src="https://open.spotify.com/embed/track/5WMKS1iDfugyLhfibIlR51" width="300" height="380" frameborder="0" allowtransparency="true" allow="encrypted-media"></iframe>

This performance shows a clear line at 110 bpm. The actual bpm is around 40 so this is clearly the tempo octave.

Introduction
=============================================================================================

### Introduction: Musical Copycats {data-commentary-width=400}
<h1> What is this project about? </h1>
A lot of the times people compare guitarists to what they deem the greatest guitarist of all time and often this guitarist is Jimi Hendrix. Hendrix was an innovative guitarist who changed the musical landscape forever with his unique style of playing. With such a huge legacy and influence it is no surprise that people wanted to emulate his style. <br>
Sometimes this emulating goes a bit far and people accuse others of actually copying Hendrix' style. For this project I wanted to take a closer look at some of these so-called "copy cats" to see if these statements have some truth to them. I have chosen to examine Stevie Ray Vaughan and John Mayer as they often get compared to Jimi Hendrix. <br> <br>

The main question for this project will be: <br>
*How similar are "copy cats of Jimi Hendrix" (such as Stevie Ray Vaughan and John Mayer) to Jimi Hendrix himself?*




Data
==============================================================================================

Row
-----------------------------------------------------------------------------------------------

To compare these different artist and their playing style, live performances were gathered and put into spotify playlists. The numbers of songs for Jimi Hendrix, John Mayer and Stevie Ray Vaughan are 158, 92 and 116 respectively. The code below shows how these playlist were than put together. These playlist represent the artist live perfomance playing styles and should be sufficient to compare the artist to eachother.

```{r echo = TRUE}
hendrix <- get_playlist_audio_features('bartderooij2009', '5HDlUKMqBToiJdBpMbRQtM')
srv <- get_playlist_audio_features('bartderooij2009', '4QXGoJ3i6ThCBw4b7DOZ8j')
mayer <- get_playlist_audio_features('bartderooij2009', '5OsbIHtbhXQ7FBNDN1HblL')

live_performances <-
  hendrix %>% mutate(playlist = "Jimi Hendrix") %>%
  bind_rows(srv %>% mutate(playlist = "Stevie Ray Vaughan")) %>%
  bind_rows(mayer %>% mutate(playlist = "John Mayer"))
```


Row
----------------------------------------------------------------------------------------------------
```{r}
counts <- live_performances %>%
group_by(playlist) %>%
  summarize(
    count = n())
```

### Chart 1
```{r}
valueBox(value=filter(counts, playlist == "Jimi Hendrix")[2],
         caption='live performances by Jimi Hendrix',
         icon = "ion-musical-notes-outline")
```

### Chart 2
```{r}
valueBox(value=filter(counts, playlist == "John Mayer")[2],
         caption="live performances by John Mayer",
         icon = "ion-musical-notes-outline")
```

### Chart 3
```{r}
valueBox(value=filter(counts, playlist == "Stevie Ray Vaughan")[2],
         caption = "live performances by Stevie Ray Vaughan",
         icon = "ion-musical-notes-outline")
```


Analysis {.storyboard}
==============================================================================================

### Scatterplot of all songs {data-commentary-width=400}
```{r, fig.width=15, fig.height=8}
# Some labels for specific songs
live_labels <-
  tibble(
    label = c("Voodoo Child\n (Slight Return)", "Voodoo Child\n (Slight Return)", "Wait Untill \nTomorrow", "Lenny", "Lenny"),
    playlist = c("Jimi Hendrix", "Stevie Ray Vaughan", "John Mayer", "Stevie Ray Vaughan", "John Mayer"),
    valence = c(0.695, 0.351, 0.738,0.274 , 0.352),
    energy = c(0.645, 0.784, 0.908,0.248 , 0.315),
  )

# Scatter plot
scatter <- live_performances %>%
  mutate(instrumentalness = ifelse(instrumentalness >= 0.5, 'No Vocals', 'With Vocals')) %>%
  ggplot(aes(x = valence, y = energy, fill = instrumentalness, size = danceability)) +
  geom_jitter(alpha = 0.45, shape = 21, color = "gray") +
  geom_text(                   # Add text labels from above.
    aes(
      x = valence,
      y = energy,
      label = label),
    colour = "black",        # Override colour (not mode here).
    size = 3,                # Override size (not loudness here).
    data = live_labels,     # Specify the data source for labels.
    hjust = "left",          # Align left side of label with the point.
    vjust = "bottom",        # Align bottom of label with the point.
    nudge_x = -0.05,         # Nudge the label slightly left.
    nudge_y = 0.02,           # Nudge the label slightly up.
    inherit.aes = FALSE
  ) +
  facet_wrap(~ playlist) +
  theme_linedraw() +
  scale_x_continuous(          # Fine-tune the x axis.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),  # Use grid-lines for quadrants only.
    minor_breaks = NULL      # Remove 'minor' grid-lines.
  ) +
  scale_y_continuous(          # Fine-tune the y axis in the same way.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),
    minor_breaks = NULL
  ) +
  scale_size_continuous(       # Fine-tune the sizes of each point.
    trans = "identity",           # Use an exp transformation to emphasise loud.
    guide = "none"           # Remove the legend for size.
  ) +
  scale_fill_manual(values = c("#d9324b", "#42e375"), name= "Vocals") +
  labs(                        # Make the titles nice.
    x = "Valence",
    y = "Energy",
    fill = "Vocals"
  )
scatter
```
***

For comparing the full data set, the valence and energy are plotted against eachother. The size of the dots is represented by the danceability of the tracks. The plot can be seen below.

### Histogram of keys {data-commentary-width=400}
```{r, fig.width=15, fig.height=8}
# Bar plot
bar <- live_performances %>%
group_by(playlist, key_name) %>%
  summarize(
    count = n()) %>%
  mutate(count = ifelse(playlist == "Jimi Hendrix", count/158*100,
                    ifelse(playlist == "Stevie Ray Vaughan", count/116*100,
                    ifelse(playlist == "John Mayer", count/92 * 100, "no")))) %>%
  ggplot(aes(x = key_name, y = count, fill = playlist)) +
  geom_col(position = position_dodge(width = 0.6), alpha = 0.6, width = 1.2) +
  ggtitle("Percentage of songs per key") +
  labs(                        # Make the titles nice.
    x = "Key Signature",
    y = "Percentage %",
    fill = "Artist"
  ) +
  theme_light() +
  scale_y_continuous(          # Fine-tune the y axis in the same way.
    limits = c(0, 26),
    breaks = c(0, 5, 10, 15, 20, 25),
    minor_breaks = NULL
  ) +
  scale_fill_brewer(type = "qual", palette = "Paired")
bar
```

***
Each track by spotify has a key signature assigned to it. When the number of songs for each key signature is counted, the following plot is created.


### Comparing Performances {data-commentary-width=400}

<div>

```{r}
 voodoo_hendrix <-
  get_tidy_audio_analysis('4HgTw7wN3mhX8nZOOLVLoO') %>%
  select(segments) %>% unnest(segments) %>%
  select(start, duration, pitches)

voodoo_srv <-
  get_tidy_audio_analysis('0vEtfYh0hj9XDRs3QAAhIT') %>%
  select(segments) %>% unnest(segments) %>%
  select(start, duration, pitches)

lenny_srv <-
  get_tidy_audio_analysis('0L3jwveF4FBMhzseOHyDyh') %>%
  select(segments) %>% unnest(segments) %>%
  select(start, duration, pitches)

lenny_mayer <-
  get_tidy_audio_analysis('2auWdKsT2wvdGIXjxBYvpS') %>%
  select(segments) %>% unnest(segments) %>%
  select(start, duration, pitches)

```

```{r, fig.width=15, fig.height=8}
compmus_long_distance(
  voodoo_hendrix %>% mutate(pitches = map(pitches, compmus_normalise, 'chebyshev')),
  voodoo_srv %>% mutate(pitches = map(pitches, compmus_normalise, 'chebyshev')),
  feature = pitches,
  method = 'euclidean') %>%
  ggplot(mapping =
    aes(
      x = (xstart + xduration / 2),
      width = xduration,
      y = (ystart + yduration / 2),
      height = yduration,
      fill = d)) +
  geom_tile() +
  scale_fill_continuous(type = 'viridis', guide = 'none') +
  labs(x = 'Jimi Hendrix',
       y = 'Stevie Ray Vaughan', title = "Voodoo Child (Slight Return) comparion")
```
```{r, fig.width=15, fig.height=8}
compmus_long_distance(
  lenny_mayer %>% mutate(pitches = map(pitches, compmus_normalise, 'chebyshev')),
  lenny_srv %>% mutate(pitches = map(pitches, compmus_normalise, 'chebyshev')),
  feature = pitches,
  method = 'euclidean') %>%
  ggplot(mapping =
    aes(
      x = (xstart + xduration / 2),
      width = xduration,
      y = (ystart + yduration / 2),
      height = yduration,
      fill = d)) +
  geom_tile() +
  scale_fill_continuous(type = 'viridis', guide = 'none') +
  labs(x = 'John Mayer',
       y = 'Stevie Ray Vaughan', title = "Lenny Comparison")
```

</div>
***
It is also important to inspect individual performances of the artist to compare them. In this section a performance of Jimi Hendrix and a performance of Stevie Ray Vaughan of Voodoo Child (Slight Return) is compared by their pitches. The same was done below that for a performance of Lenny by John Mayer and Stevie Ray Vaughan.

### Self-matrices based on timbre and pitches {data-commentary-width=400}
```{r}
make_self_similarity_matrix <- function(track_id, method="timbre"){
  track <-
    get_tidy_audio_analysis(track_id) %>%
    compmus_align(bars, segments) %>%
    select(bars) %>% unnest(bars) %>%
    mutate(
      pitches =
        map(segments,
            compmus_summarise, pitches,
            method = 'rms', norm = 'euclidean')) %>%
    mutate(
      timbre =
        map(segments,
            compmus_summarise, timbre,
            method = 'rms', norm = 'euclidean'))

  if (method == "pitches"){
    plot <- track %>%
      compmus_self_similarity(pitches, 'euclidean') %>%
      ggplot(
        aes(
          x = xstart + xduration / 2,
          width = xduration,
          y = ystart + yduration / 2,
          height = yduration,
          fill = d)) +
      geom_tile() +
      coord_fixed() +
      scale_fill_viridis_c(option = 'E', guide = 'none') +
      theme_classic() +
      ggtitle("Pitch-based self-similarity matrix") +
      labs(x = '', y = '')
  }

  if (method == "timbre"){
    plot <- track %>%
      compmus_self_similarity(timbre, 'euclidean') %>%
      ggplot(
        aes(
          x = xstart + xduration / 2,
          width = xduration,
          y = ystart + yduration / 2,
          height = yduration,
          fill = d)) +
      geom_tile() +
      coord_fixed() +
      scale_fill_viridis_c(option = 'E', guide = 'none') +
      theme_classic() +
      ggtitle("Timbre-based self-similarity matrix") +
      labs(x = '', y = '')
  }
  return(plot)
}
```



<div>
```{r, fig.width=12, fig.height=8}
hendrix_voodoo_timbre <- make_self_similarity_matrix('5rywNTqQPofdsof7f5gvY4', "timbre")
hendrix_voodoo_pitches <- make_self_similarity_matrix('5rywNTqQPofdsof7f5gvY4', "pitches")

grid.arrange(hendrix_voodoo_pitches, hendrix_voodoo_timbre, ncol=2,
top = textGrob("Jimi Hendrix - Voodoo Child (Slight Return) Live",gp=gpar(fontsize=28,font=1)))
```
</div>

<div>
```{r, fig.width=12, fig.height=8}
srv_pnj_timbre <- make_self_similarity_matrix('4TqS8mahRles9lf97hZ49X', "timbre")
srv_pnj_pitches <- make_self_similarity_matrix('4TqS8mahRles9lf97hZ49X', "pitches")

grid.arrange(srv_pnj_pitches, srv_pnj_timbre, ncol=2,
top = textGrob("Stevie Ray Vaughan - Voodoo Child (Slight Return) Live",gp=gpar(fontsize=28,font=1)))
```
</div>

<div>
```{r, fig.width=12, fig.height=8}
mayer_wait_until_timbre <- make_self_similarity_matrix('5SSqUpMby3y8W0qy3JME5E', "timbre")
mayer_wait_until_pitches <- make_self_similarity_matrix('5SSqUpMby3y8W0qy3JME5E', "pitches")

grid.arrange(mayer_wait_until_pitches, mayer_wait_until_timbre, ncol=2,
top = textGrob("John Mayer - Wait Until Tomorrow Live",gp=gpar(fontsize=28,font=1)))
```
</div>

***

The three plots on the left show the pitch-based and timbre-based self-similarity matrices for a performance of each artist. The plots for Jimi Hendrix show that there is not a real strict chorus-verse structure in his performance. This was the case in more recordings which makes it hard to draw conclusions from the plots. Stevie Ray Vaughan seems to follow some reoccurring structure. Especially in the timbre plot, there is a distinction between low and high energy areas. John Mayer has the most structured plots in which we can distinguish some verses and choruses. At around 100 seconds it even shows some diagonal dark lines which indicate similar sections. <br><br>
When searching for songs to plot, there seemed to be a reoccurring problem of fuzzy plots. This is probably caused by the fact that the recordings are live. Especially the Jimi Hendrix performances are messy which makes it hard to really compare the artists. I will try to experiment with some different playlists in the coming week(s) to see if I can circumvent these problems.

```{r}
circshift <- function(v, n) {if (n == 0) v else c(tail(v, n), head(v, -n))}

# C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)
chord_templates <-
  tribble(
    ~name  , ~template,
    'Gb:7'  , circshift(seventh_chord,  6),
    'Gb:maj', circshift(major_chord,    6),
    'Bb:min', circshift(minor_chord,   10),
    'Db:maj', circshift(major_chord,    1),
    'F:min' , circshift(minor_chord,    5),
    'Ab:7'  , circshift(seventh_chord,  8),
    'Ab:maj', circshift(major_chord,    8),
    'C:min' , circshift(minor_chord,    0),
    'Eb:7'  , circshift(seventh_chord,  3),
    'Eb:maj', circshift(major_chord,    3),
    'G:min' , circshift(minor_chord,    7),
    'Bb:7'  , circshift(seventh_chord, 10),
    'Bb:maj', circshift(major_chord,   10),
    'D:min' , circshift(minor_chord,    2),
    'F:7'   , circshift(seventh_chord,  5),
    'F:maj' , circshift(major_chord,    5),
    'A:min' , circshift(minor_chord,    9),
    'C:7'   , circshift(seventh_chord,  0),
    'C:maj' , circshift(major_chord,    0),
    'E:min' , circshift(minor_chord,    4),
    'G:7'   , circshift(seventh_chord,  7),
    'G:maj' , circshift(major_chord,    7),
    'B:min' , circshift(minor_chord,   11),
    'D:7'   , circshift(seventh_chord,  2),
    'D:maj' , circshift(major_chord,    2),
    'F#:min', circshift(minor_chord,    6),
    'A:7'   , circshift(seventh_chord,  9),
    'A:maj' , circshift(major_chord,    9),
    'C#:min', circshift(minor_chord,    1),
    'E:7'   , circshift(seventh_chord,  4),
    'E:maj' , circshift(major_chord,    4),
    'G#:min', circshift(minor_chord,    8),
    'B:7'   , circshift(seventh_chord, 11),
    'B:maj' , circshift(major_chord,   11),
    'D#:min', circshift(minor_chord,    3))

key_templates <-
  tribble(
    ~name    , ~template,
    'Gb:maj', circshift(major_key,  6),
    'Bb:min', circshift(minor_key, 10),
    'Db:maj', circshift(major_key,  1),
    'F:min' , circshift(minor_key,  5),
    'Ab:maj', circshift(major_key,  8),
    'C:min' , circshift(minor_key,  0),
    'Eb:maj', circshift(major_key,  3),
    'G:min' , circshift(minor_key,  7),
    'Bb:maj', circshift(major_key, 10),
    'D:min' , circshift(minor_key,  2),
    'F:maj' , circshift(major_key,  5),
    'A:min' , circshift(minor_key,  9),
    'C:maj' , circshift(major_key,  0),
    'E:min' , circshift(minor_key,  4),
    'G:maj' , circshift(major_key,  7),
    'B:min' , circshift(minor_key, 11),
    'D:maj' , circshift(major_key,  2),
    'F#:min', circshift(minor_key,  6),
    'A:maj' , circshift(major_key,  9),
    'C#:min', circshift(minor_key,  1),
    'E:maj' , circshift(major_key,  4),
    'G#:min', circshift(minor_key,  8),
    'B:maj' , circshift(major_key, 11),
    'D#:min', circshift(minor_key,  3))

get_chordogram <- function(uri, chord_templates){
  plot <- get_tidy_audio_analysis(uri) %>%
    compmus_align(bars, segments) %>%
    select(bars) %>% unnest(bars) %>%
    mutate(
      pitches =
        map(segments,
            compmus_summarise, pitches,
            method = 'mean', norm = 'manhattan'))  %>%
    compmus_match_pitch_template(chord_templates, 'euclidean', 'manhattan') %>%
    ggplot(
      aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    theme_minimal() +
    labs(x = 'Time (s)', y = '')
}

get_keygram <- function(uri, key_templates){
  plot <- get_tidy_audio_analysis(uri) %>%
    compmus_align(sections, segments) %>%
    select(sections) %>% unnest(sections) %>%
    mutate(
      pitches =
        map(segments,
            compmus_summarise, pitches,
            method = 'mean', norm = 'manhattan'))  %>%
    compmus_match_pitch_template(chord_templates, 'euclidean', 'manhattan') %>%
    ggplot(
      aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    theme_minimal() +
    labs(x = 'Time (s)', y = '')
}
```

### Jimi Hendrix' Bold as Love rhythm guitar causes key ambiguity {data-commentary-width=320}

<div>
<h3>Chordogram</h3>
```{r, out.width="100%", fig.width=15}
boldhendrix_chords <- get_chordogram('0uco0wQkB909zpPlHvu5Cc', chord_templates)
boldhendrix_chords
```
</div>

<div>
<h3>Keygram</h3>
```{r, out.width="100%", fig.width=15}
boldhendrix_keys <- get_chordogram('0uco0wQkB909zpPlHvu5Cc', key_templates)
boldhendrix_keys
```
</div>


***

<iframe src="https://open.spotify.com/embed/track/0uco0wQkB909zpPlHvu5Cc" width="300" height="380" frameborder="0" allowtransparency="true" allow="encrypted-media"></iframe>

The keygram of Bold as Love from Jimi Hendrix does not indicate a clear key. What we can clearly see is that there is a fade-out in the end. The chords from the chordogram are also not very clear. This is probably caused by Hendrix' style of rhythm guitar which uses a lot of embellishments.

### Which can also be seen in John Mayers version {data-commentary-width=320}
<div>
<h3>Chordogram</h3>
```{r, out.width="100%", fig.width=15}
boldmayer_chords <- get_chordogram('50DMG6AvtNzbrxMc62w6ph', chord_templates)
boldmayer_chords
```
</div>

<div>
<h3>Keygram</h3>
```{r, out.width="100%", fig.width=15}
boldmayer_keys <- get_chordogram('50DMG6AvtNzbrxMc62w6ph', key_templates)
boldmayer_keys
```
</div>

***

<iframe src="https://open.spotify.com/embed/track/50DMG6AvtNzbrxMc62w6ph" width="300" height="380" frameborder="0" allowtransparency="true" allow="encrypted-media"></iframe>

We see that this version of Bold as Love by John Mayer shows very much the same pattern as the version of Jimi Hendrix. We do see some sharper patterns that the Hendrix version which could be caused by the very different EQs of the versions. The Hendrix version has a darker sound quality to it while Mayers version is brighter.

### All Along the Watchtower is less ambiguous because of the more straight forward rhythm section {data-commentary-width=320}

<div>
<h3>Chordogram</h3>
```{r, out.width="100%", fig.width=15}
watchhendrix_chords <- get_chordogram('2aoo2jlRnM3A0NyLQqMN2f', chord_templates)
watchhendrix_chords
```
</div>
<div>
<h3>Keygram</h3>
```{r, out.width="100%", fig.width=15}
watchhendrix_keys <- get_chordogram('2aoo2jlRnM3A0NyLQqMN2f', key_templates)
watchhendrix_keys
```
</div>

***

<iframe src="https://open.spotify.com/embed/track/2aoo2jlRnM3A0NyLQqMN2f" width="300" height="380" frameborder="0" allowtransparency="true" allow="encrypted-media"></iframe>

In this version of All Along the Watchtower by Jimi Hendrix we can clearly see chords and keys which are more distinct. The chords from the song are mainly Cm, Bb and Ab. It is noteable that Dmaj is dark throughout the whole song.

### The same ambiguity arises with Stevie Ray Vaughans Little Wing {data-commentary-width=320}
<div>
<h3>Chordogram</h3>
```{r, out.width="100%", fig.width=15}
littlewing_chords <- get_chordogram('593SUshyuymkyDYbLVBMfr', chord_templates)
littlewing_chords
```
</div>

<div>
<h3>Keygram</h3>
```{r, out.width="100%", fig.width=15}
littlewing_keys <- get_chordogram('593SUshyuymkyDYbLVBMfr', key_templates)
littlewing_keys
```
</div>

***

<iframe src="https://open.spotify.com/embed/track/593SUshyuymkyDYbLVBMfr" width="300" height="380" frameborder="0" allowtransparency="true" allow="encrypted-media"></iframe>

For this version from Little Wing by Stevie Ray Vaughan, we can see the same problems. No clear key can be found because of there is no rigid structure of chords in the background. From the chordogram we can see some reocurring chords including Gm and Bb7.

Discussion
==============================================================================================

From these minimal comparisons it is still hard to draw conclusions because more research needs to be done. This section will discuss the plots in short. <br><br>

From the plots we can see that the Mayer plot has the most danceable tracks and that the bulk of the data is in the middle of the plot. Another thing that is noticeable, is that Jimi Hendrix and Stevie Ray Vaughan have a lot more instrumental songs. <br>
Similar songs and covers of these songs give similar valence and energy results (Lenny, Voodoo Child, Wait Until Tomorrow).<br>
From the key signature plot we see that Jimi Hendrix and Stevie Ray Vaughan have a similar distribution of key signatures they play in.
